# Parcial-2-IA
## Procesamiento de Datos
En este proyecto utilizamos el dataset Adult Census Income proveniente del repositorio UCI, el cual contiene información demográfica y socioeconómica de individuos en Estados Unidos. La variable objetivo original income estaba representada con valores textuales como "<=50K", ">50K" en el archivo de prueba, también habia versiones que incluían puntos finales como ">50K.". Para evitar inconsistencias, se realizamos una limpieza unificada de esta columna, transformándola finalmente a un formato binario donde "<=50K" corresponde al valor 0 y ">50K" corresponde al valor 1.
El dataset de prueba original fue dividido en dos partes equivalentes (50/50), con el propósito de construir un conjunto de validación y un conjunto de prueba final. De esta manera, se garantizó un proceso de entrenamiento, validación y evaluación más riguroso, minimizando el riesgo de sobreajuste.
En cuanto al preprocesamiento de variables, se decidió mantener todas las características originales del dataset, sin eliminar ni generar nuevas variables. Las variables de tipo numérico las normalizamos utilizando StandardScaler, con el objetivo de centrar y escalar los valores para que todas las magnitudes fueran comparables y facilitar el entrenamiento de los modelos. Por su parte, las variables categóricas fueron transformadas mediante OneHotEncoder, lo que permitió representarlas como vectores binarios sin introducir un orden artificial en las categorías.
El resultado final del preprocesamiento puede observarse en las primeras filas de la base transformada, donde tanto las características categóricas como las numéricas aparecen como valores estandarizados o codificados en 0 y 1, acompañadas de la variable objetivo en formato binario.

## Experimentos con MLP

Para la construcción de redes neuronales utilizamos un MLP en PyTorch, implementando una arquitectura flexible con capas ocultas, funciones de activación ReLU, dropout como técnica de regularización y una capa final con función sigmoide para producir probabilidades asociadas a la clase positiva. El modelo fue entrenado con la función de pérdida de entropía cruzada binaria BCELoss y utilizamos el optimizador Adam, que permitió un ajuste eficiente de los parámetros. Además, se implementó un mecanismo de Early Stopping, de manera que el entrenamiento se interrumpiera automáticamente en caso de no observar mejoras en la pérdida de validación durante varios ciclos, evitando así sobreajuste.
Se llevaron a cabo al menos cinco experimentos con configuraciones distintas de hiperparámetros. Entre las variaciones exploradas se incluyeron el número de capas ocultas, la cantidad de neuronas por capa, la tasa de dropout, la tasa de aprendizaje del optimizador y el factor de regularización por medio de weight decay. Cada modelo se entrenó durante un máximo de 50 épocas, aunque en la práctica la técnica de Early Stopping detuvo la mayoría de los entrenamientos antes de alcanzar dicho límite.
Las gráficas de pérdida (loss) en entrenamiento y validación fueron un insumo clave para el análisis. En algunos experimentos se evidenció un comportamiento típico de overfitting, en el que la pérdida de entrenamiento continuaba descendiendo mientras que la pérdida de validación se estancaba o incluso aumentaba. En otros casos, con configuraciones más regulares, se observó un mejor equilibrio entre ambas curvas, lo que permitió identificar arquitecturas con mayor capacidad de generalización.
De todos los modelos probados, el mejor desempeño se alcanzó con una configuración que combinaba múltiples capas ocultas con un valor intermedio de dropout y un factor moderado de regularización. Este modelo no solo presentó curvas de pérdida estables, sino que también logró mejorar las métricas de validación en comparación con el baseline de regresión logística, lo que confirma la importancia de ajustar hiperparámetros de manera cuidadosa. La comparación entre el MLP sin regularización y el MLP con regularización mostró claramente que la inclusión de dropout y weight decay permitió controlar el sobreajuste y obtener un rendimiento más robusto en los datos de validación y prueba.
